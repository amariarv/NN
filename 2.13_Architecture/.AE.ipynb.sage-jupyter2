{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":1644019712},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1595322855860,"exec_count":1,"id":"b889ad","input":"def get_free_gpu():\n    from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetCount\n    nvmlInit()\n\n    return np.argmax([\n        nvmlDeviceGetMemoryInfo(nvmlDeviceGetHandleByIndex(i)).free\n        for i in range(nvmlDeviceGetCount())\n    ])","kernel":"python3","pos":1,"start":1595322855856,"state":"done","type":"cell"}
{"cell_type":"code","end":1595322863907,"exec_count":2,"id":"1c8504","input":"import numpy as np\nimport torch\nimport torchvision\n\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\n\nif torch.cuda.is_available():\n    cuda_id = get_free_gpu()\n    DEVICE = 'cuda:%d' % (get_free_gpu(), )\n    print('Selected %s' % (DEVICE, ))\nelse:\n    DEVICE = 'cpu'\n    print('WARNING: using cpu!')\n\n### please, don't remove the following line\nx = torch.tensor([1], dtype=torch.float32).to(DEVICE)","kernel":"python3","output":{"0":{"name":"stdout","text":"Selected cuda:3\n"}},"pos":2,"start":1595322859836,"state":"done","type":"cell"}
{"cell_type":"code","end":1595322867086,"exec_count":3,"id":"091d4b","input":"from torchvision.datasets import MNIST\n\nds_train = MNIST(\"../../data/\", train=True, download=True)\nds_test = MNIST(\"../../data/\", train=False, download=True)\n\ndata_train = ds_train.data.reshape(-1, 1, 28, 28).cpu().numpy().astype(np.float32) / 255\n\ndataset_test = torch.utils.data.DataLoader(\n    torch.utils.data.TensorDataset(\n        (ds_test.data.reshape(-1, 1, 28, 28).to(torch.float32) / 255).to(DEVICE)\n    ),\n    batch_size=32\n)","kernel":"python3","pos":4,"start":1595322866946,"state":"done","type":"cell"}
{"cell_type":"code","end":1595322873720,"exec_count":4,"id":"40e7cc","input":"plt.figure(figsize=(12, 6), dpi=100)\nplt.axis('off')\n_ = plt.imshow(\n    np.concatenate(\n        np.concatenate(data_train[:200].reshape(20, 10, 28, 28), axis=2),\n        axis=0\n    ),\n    cmap=plt.cm.Greys\n)","kernel":"python3","output":{"0":{"data":{"image/png":"22e1d7c13fe504158dd3f2ef49a514b6c1fe9537","text/plain":"<Figure size 1200x600 with 1 Axes>"}}},"pos":5,"start":1595322873229,"state":"done","type":"cell"}
{"cell_type":"code","end":1595322889028,"exec_count":5,"id":"7c15bf","input":"class View(torch.nn.Module):\n    def __init__(self, *shape):\n        super(View, self).__init__()\n        self.shape = shape\n\n    def forward(self, x):\n        return x.view(*self.shape)","kernel":"python3","pos":7,"start":1595322889025,"state":"done","type":"cell"}
{"cell_type":"code","end":1595323088178,"exec_count":6,"id":"500f41","input":"n = 8\n\ncode_size = 4 * n\n\nencoder = torch.nn.Sequential(\n    ### 26 x 26\n    torch.nn.Conv2d(1, 2 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n    ### 12 x 12\n    torch.nn.Conv2d(2 * n, 2 * n, kernel_size=3, stride=2), torch.nn.LeakyReLU(),\n    \n    ### 10 x 10\n    torch.nn.Conv2d(2 * n, 3 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n    ### 4 x 4\n    torch.nn.Conv2d(3 * n, 3 * n, kernel_size=3, stride=2), torch.nn.LeakyReLU(),\n    \n     ### 2 x 2\n     torch.nn.Conv2d(3 * n, 4 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n\n     torch.nn.Flatten(),\n     torch.nn.Linear(2 * 2 * 4 * n, code_size),\n).to(DEVICE)\n\ndecoder = torch.nn.Sequential(\n    torch.nn.Linear(code_size, 2 * 2 * 4 * n),\n    View(-1, 4 * n, 2, 2),\n    \n    ### 4 x 4\n    torch.nn.ConvTranspose2d(4 * n, 3 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n    \n    ### 10 x 10\n    torch.nn.ConvTranspose2d(3 * n, 3 * n, kernel_size=3, stride=2, output_padding=1), torch.nn.LeakyReLU(),\n    ### 12 x 12\n    torch.nn.ConvTranspose2d(3 * n, 2 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n    \n    ### 24 x 24\n    torch.nn.ConvTranspose2d(2 * n, 2 * n, kernel_size=3, stride=2, output_padding=1), torch.nn.LeakyReLU(),\n    torch.nn.ConvTranspose2d(2 * n, 1, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n).to(DEVICE)","kernel":"python3","pos":8,"start":1595323088171,"state":"done","type":"cell"}
{"cell_type":"code","end":1595323167102,"exec_count":8,"id":"3c2fb4","input":"opt = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()))","kernel":"python3","pos":10,"start":1595323167098,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"1c5cca","input":"### Decoder, however, now receives an additional input - angle of rotation\nclass Decoder(torch.nn.Module):\n    def __init__(self, n, code_size):\n        \n        super(Decoder, self).__init__()\n        \n        self.modules = [\n            torch.nn.Linear(code_size + 1, 2 * 2 * 4 * n),\n            View(-1, 4 * n, 2, 2),\n\n            ### 4 x 4\n            torch.nn.ConvTranspose2d(4 * n, 3 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n\n            ### 10 x 10\n            torch.nn.ConvTranspose2d(3 * n, 3 * n, kernel_size=3, stride=2, output_padding=1), torch.nn.LeakyReLU(),\n            ### 12 x 12\n            torch.nn.ConvTranspose2d(3 * n, 2 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n\n            ### 24 x 24\n            torch.nn.ConvTranspose2d(2 * n, 2 * n, kernel_size=3, stride=2, output_padding=1), torch.nn.LeakyReLU(),\n            torch.nn.ConvTranspose2d(2 * n, 1, kernel_size=3, stride=1), torch.nn.LeakyReLU()\n        ]\n        \n        for i, f in enumerate(self.modules):\n            self.add_module('f%d' % (i, ), f)\n    \n    def forward(self, code, angle): # the angle is what I need to return the rotation from\n        code = torch.cat([code, angle], axis=1)\n        \n        x = code\n        for f in self.modules:\n            x = f(x)\n        \n        return x","pos":31,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"2817fe","input":"### this AE needs more capacity...\nn = 16\ncode_size = 64\n\nencoder_rot = Encoder(n,  code_size).to(DEVICE)\ndecoder_rot = Decoder(n,  code_size).to(DEVICE)","pos":32,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"37ff16","input":"n_cols = 20\nn_rows = 10\nX = torch.tensor(data_train[:2 * n_rows], device=DEVICE, dtype=torch.float32, requires_grad=False)\n\nwith torch.no_grad():\n    code = encoder(X)\n    code1, code2 = code[:n_rows], code[n_rows:]\n    alphas = np.linspace(0, 1, num=n_cols)\n\n    results = np.stack([\n        decoder(alpha * code1 + (1 - alpha) * code2).cpu().numpy()\n        for alpha in alphas\n    ], axis=0)\n\nplt.figure(figsize=(2 * n_cols, 2 * n_rows))\nplt.axis('off')\nplt.imshow(\n    np.concatenate(\n        np.concatenate(results, axis=-1),\n        axis=-2\n    )[0, :, :],\n    cmap=plt.cm.Greys\n)","pos":18,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"4ec191","input":"rng = np.random.RandomState(seed=1111)\n\nwith torch.no_grad():\n    errors = list()\n    \n    for X, in dataset_test:\n        X_rot = torch.tensor(\n            rotate(X.cpu().numpy(), np.random.uniform(0, 360, size=X.shape[0])),\n            device=DEVICE, dtype=torch.float32, requires_grad=False\n        )\n    \n        errors.append(\n            torch.mean((X - decoder(encoder(X))) ** 2, axis=(1, 2, 3)).cpu().numpy()\n        )\n\nerrors = np.concatenate(errors, axis=0)\n    \nassert np.mean(errors) < 1e-2, 'Reconstruction error is too large %.3lf' % (np.mean(errors), )","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"AE-rotation-test","locked":true,"points":1,"schema_version":3,"solution":false,"task":false}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"517e96","input":"def rotate(imgs, angles):\n    from PIL import Image\n\n    return np.stack([\n        np.array(\n            Image.fromarray(img[0, :, :]).rotate(angle, Image.BILINEAR)\n        )\n        for img, angle in zip(imgs, angles)\n    ], axis=0).reshape(-1, 1, 28, 28)","pos":20,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"53376d","input":"batch_size = 32\nn_epoches = 8\nn_batches = len(data_train) // batch_size\n\nlosses = np.zeros((n_epoches, n_batches), dtype=np.float32)\n\nfor i in tqdm(range(n_epoches)):\n    for j in range(n_batches):\n        # YOUR CODE HERE\n        raise NotImplementedError()","metadata":{"nbgrader":{"cell_type":"code","checksum":"bc0a10f5a008d9489e8245d95b229c5a","grade":false,"grade_id":"AE-optional","locked":false,"schema_version":3,"solution":true,"task":false}},"pos":36,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"66612a","input":"X = torch.tensor(data_train[:20], device=DEVICE, dtype=torch.float32, requires_grad=False)\nX_rec = decoder(encoder(X)).cpu().detach().numpy()\n\nplt.figure(figsize=(40, 2))\nplt.axis('off')\nplt.imshow(\n    np.concatenate(\n        np.concatenate([data_train[:20], X_rec], axis=2),\n        axis=2\n    )[0, :, :],\n    cmap=plt.cm.Greys\n)","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"689812","input":"### Encoder stays, essentially, the same\nclass Encoder(torch.nn.Module):\n    def __init__(self, n, code_size):\n        \n        super(Encoder, self).__init__()\n        \n        self.modules = [\n            ### 26 x 26\n            torch.nn.Conv2d(1, 2 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n            ### 12 x 12\n            torch.nn.Conv2d(2 * n, 2 * n, kernel_size=3, stride=2), torch.nn.LeakyReLU(),\n\n            ### 10 x 10\n            torch.nn.Conv2d(2 * n, 3 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n            ### 4 x 4\n            torch.nn.Conv2d(3 * n, 3 * n, kernel_size=3, stride=2), torch.nn.LeakyReLU(),\n\n             ### 2 x 2\n             torch.nn.Conv2d(3 * n, 4 * n, kernel_size=3, stride=1), torch.nn.LeakyReLU(),\n\n             torch.nn.Flatten(),\n        ]\n        \n        for i, f in enumerate(self.modules):\n            self.add_module('f%d' % (i, ), f)\n\n        self.code = torch.nn.Linear(2 * 2 * 4 * n, code_size)\n\n    def forward(self, x):\n        for f in self.modules:\n            x = f(x)\n        \n        return self.code(x)","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"6abf80","input":"","pos":40,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7dde6a","input":"plt.plot(np.mean(losses, axis=1), label='MSE')\nplt.xlabel('iteration')","pos":13,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"857f6d","input":"opt_split = torch.optim.Adam(list(encoder_rot.parameters()) + list(decoder_rot.parameters()))","pos":35,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8b7349","input":"batch_size = 32\nn_epoches = 4\nn_batches = len(data_train) // batch_size\n\nlosses = np.zeros((n_epoches, n_batches), dtype=np.float32)\n\nfor i in tqdm(range(n_epoches)):\n    for j in range(n_batches):\n        indx = np.random.randint(0, data_train.shape[0], size=batch_size)\n        imgs = data_train[indx]\n\n        # YOUR CODE HERE\n        raise NotImplementedError()\n        \n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n        \n        losses[i, j] = loss.item()","metadata":{"nbgrader":{"cell_type":"code","checksum":"5af7efbd2d81c5e1565fad92ebc4625c","grade":false,"grade_id":"AE-rotation","locked":false,"schema_version":3,"solution":true,"task":false}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"93e8e1","input":"X_rot = rotate(data_train[:200], np.random.uniform(0, 360, size=200))\n\nplt.figure(figsize=(12, 6), dpi=100)\nplt.axis('off')\n_ = plt.imshow(\n    np.concatenate(\n        np.concatenate(X_rot.reshape(20, 10, 28, 28), axis=2),\n        axis=0\n    ),\n    cmap=plt.cm.Greys\n)","pos":21,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ae08db","input":"with torch.no_grad():\n    errors = np.concatenate([\n        torch.mean((X - decoder(encoder(X))) ** 2, axis=(1, 2, 3)).cpu().numpy()\n        for X, in dataset_test\n    ], axis=0)\n    \nassert np.mean(errors) < 1e-2","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"AE","locked":true,"points":1,"schema_version":3,"solution":false,"task":false}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ae1c7d","input":"X_rot = rotate(data_train[:20], np.random.uniform(0, 360, size=200))\n\nx = torch.tensor(\n    X_rot,\n    device=DEVICE, dtype=torch.float32, requires_grad=False\n)\nx_rec = decoder(encoder(x)).cpu().detach().numpy()\n\nplt.figure(figsize=(40, 2))\nplt.axis('off')\nplt.imshow(\n    np.concatenate(\n        np.concatenate([X_rot, x_rec], axis=2),\n        axis=2\n    )[0, :, :],\n    cmap=plt.cm.Greys\n)","pos":26,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b137b3","input":"batch_size = 32\nn_epoches = 4\nn_batches = len(data_train) // batch_size\n\nlosses = np.zeros((n_epoches, n_batches), dtype=np.float32)\n\nfor i in tqdm(range(n_epoches)):\n    for j in range(n_batches):\n        ### Here, we operate with numpy tensors and then cast them to torch.tensor, but,\n        ### in general, if possible, keep your dataset in torch tensors\n        ### to avoid unnecessary CPU-to-GPU transfers,\n        ### e.g., use DataLoader with TensorDataset.\n        indx = np.random.randint(0, data_train.shape[0], size=batch_size)\n        imgs = data_train[indx]\n\n        X_batch = torch.tensor(imgs,\n                               device=DEVICE,\n                               dtype=torch.float32,\n                               requires_grad=False)\n\n        # YOUR CODE HERE\n        #raise NotImplementedError()\n#        code_batch = ...\n#        X_reconstructed = ...\n#        loss = ...\n        \n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n        losses[i, j] = loss.item()","metadata":{"nbgrader":{"cell_type":"code","checksum":"0ac4870fd5a817d0c19af0fdda2032cf","grade":false,"grade_id":"AE","locked":false,"schema_version":3,"solution":true,"task":false}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b33607","input":"n_cols = 20\nn_rows = 10\n\nX1 = torch.tensor(data_train[:n_rows], device=DEVICE, dtype=torch.float32, requires_grad=False)\nX2 = torch.tensor(\n    rotate(data_train[:n_rows], np.repeat(90, n_cols)),\n    device=DEVICE, dtype=torch.float32, requires_grad=False\n)\n\nwith torch.no_grad():\n    code1 = encoder(X1)\n    code2 = encoder(X2)\n    \n    alphas = np.linspace(0, 1, num=n_cols)\n\n    results = np.stack([\n        decoder(alpha * code1 + (1 - alpha) * code2).cpu().numpy()\n        for alpha in alphas\n    ], axis=0)\n\nplt.figure(figsize=(2 * n_cols, 2 * n_rows))\nplt.axis('off')\nplt.imshow(\n    np.concatenate(\n        np.concatenate(results, axis=-1),\n        axis=-2\n    )[0, :, :],\n    cmap=plt.cm.Greys\n)","pos":28,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b59a38","input":"opt = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()))","pos":23,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"dd0a99","input":"x = torch.tensor(data_train[:10].reshape(-1, 1, 28, 28), device=DEVICE)\nangles = torch.tensor(np.random.uniform(0, 1, size=(10, 1)).astype('float32'), device=DEVICE)\n\ncode = encoder_rot(x)\ndecoder_rot(code, angles).shape","pos":33,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"eb9286","input":"X_src = data_train[:20]\nangles = np.random.uniform(-0.5, 0.5, size=20)\nX_rot = rotate(X_src, angles * 360)\n\nwith torch.no_grad():\n    x_src = torch.tensor(X_src, device=DEVICE, dtype=torch.float32, requires_grad=False)\n    phi = torch.tensor(angles.reshape(-1, 1), device=DEVICE, dtype=torch.float32, requires_grad=False)\n    \n    code = encoder_rot(x_src)\n    X_rec = decoder_rot(code, phi).cpu().numpy()\n\nplt.figure(figsize=(40, 3))\nplt.axis('off')\nplt.imshow(\n    np.concatenate(\n        np.concatenate([X_src, X_rec, X_rot], axis=2),\n        axis=2\n    )[0, :, :],\n    cmap=plt.cm.Greys\n)","pos":38,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ee8034","input":"plt.plot(np.mean(losses, axis=1), label='MSE')\nplt.xlabel('iteration')","pos":37,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f02927","input":"n_cols = 21\nn_rows = 10\n\nX = torch.tensor(data_train[:n_rows], device=DEVICE, dtype=torch.float32, requires_grad=False)\nangles = np.linspace(-0.5, 0.5, num=n_cols)\n\nwith torch.no_grad():\n    code = encoder_rot(X)\n\n    results = np.stack([\n        decoder_rot(\n            code, torch.ones(n_rows, 1, device=DEVICE) * angle\n        ).cpu().numpy()\n        for angle in angles\n    ], axis=0)\n\n\n### replace 0 degrees rotation with the original image\nresults[n_cols // 2] = data_train[:n_rows]\n    \n    \nplt.figure(figsize=(2 * n_cols, 2 * n_rows))\nplt.axis('off')\nplt.imshow(\n    np.concatenate(\n        np.concatenate(results, axis=-1),\n        axis=-2\n    )[0, :, :],\n    cmap=plt.cm.Greys\n)","pos":39,"type":"cell"}
{"cell_type":"code","exec_count":7,"id":"6b4fad","input":"### checks if shapes are correct\nX_batch = torch.tensor(data_train[:10].reshape(-1, 1, 28, 28), device=DEVICE)\ncode = encoder(X_batch)\nX_rec = decoder(code)\n#decoder(encoder(X_batch)).shape\n\nprint(code.shape) #code size dimensionality of hidden space\nprint(X_rec.shape)","kernel":"python3","output":{"0":{"name":"stdout","text":"torch.Size([10, 32])\ntorch.Size([10, 1, 28, 28])\n"}},"pos":9,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"1cceca","input":"# Auto Encoders","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"2a29da","input":"### Algebra in latent space\n\nUnlike Eucledian space, latent space of AE supports linear algebra, i.e.:\n\n$$\\mathrm{decode}\\big[\\alpha \\cdot \\mathrm{encode}(X_1) + (1 - \\alpha) \\cdot \\mathrm{encode}(X_2)\\big]$$\n\nis not a blurry overlay of two images but a realistic image of a digit (mostly)!","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"4198bd","input":"## Task 3 (optional, not graded)\n\nFind a way to approximate image rotation by AE, i.e.:\n\n$$\\mathrm{decode}(\\mathrm{encode}(X), \\phi) = \\mathrm{rotate}(X, \\phi).$$\n\nPlease, feed decoder with angles from $[-0.5, 0.5]$ where $-0.5$ corresponds to $-180$ degrees, and $0.5$ corresponds to $180$ degrees.","pos":34,"type":"cell"}
{"cell_type":"markdown","id":"49698d","input":"## Loading data","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"667b31","input":"## Linear algebra\n\nI doesn't look like rotation is a linear operation even in the latent space... Why?","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"8a62c8","input":"### Visual check of reconstruction quality","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"912cfb","input":"## Approximating rotation (optional)\n\nHere, we aim to approximate image rotation, training AE to rotate an image by a given angle.","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"b3f491","input":"## Building AE","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"ec2401","input":"## Task 2\n\nPlease train AE like in the previous task, but make it robust to image rotations.","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"ecb27b","input":"## Augmenting data via rotations ","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"f8c8ef","input":"## Task 1\n\nPlease, complete autoencoder training procedure:\n- implement encoding;\n- implement decoding;\n- compute MSE loss.\n\nUse `loss` variable to hold value of the MSE loss on the current batch.","pos":11,"type":"cell"}
{"id":"856392","input":"n = 8 #This is less general, he added this at some point in the course\n\ncode_size = 4 * n\n\nencoder = torch.nn.Sequential(\n    torch.nn.Linear(input_size,output_size),\n).to(DEVICE)\ndecoder = torch.nn.Sequential(\n    torch.nn.Linear(outpu_size,input_size),\n).to(DEVICE)","pos":7.5,"type":"cell"}
{"id":0,"time":1595321983836,"type":"user"}
{"last_load":1595321983971,"type":"file"}