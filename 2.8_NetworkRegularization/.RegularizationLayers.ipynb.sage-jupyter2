{"backend_state":"running","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":45674496},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"trust":true,"type":"settings"}
{"cell_type":"code","id":"20b3ea","input":"import matplotlib.pyplot as plt\nimport matplotlib","pos":6,"state":"done","type":"cell"}
{"cell_type":"code","id":"21e88a","input":"batch_x, batch_y = next(iter(trainloader))\nbatch_x = batch_x[:16]\nbatch_y = batch_y[:16]\n\nplt.imshow(\n    batch_x.permute(0, 2, 3, 1).reshape(4, 4, 32, 32, 3).permute(0, 2, 1, 3, 4).reshape(128, 128, 3)\n)\nprint(\n    np.array(classes)[batch_y.numpy()].reshape(4, 4)\n)","pos":7,"state":"done","type":"cell"}
{"cell_type":"code","id":"5df48c","input":"fig, [ax0, ax1] = plt.subplots(1, 2, figsize=(12, 4), dpi=100)\n\n# per step loss values are too noizy, so we'll use a function to \n# average them with a running window\ndef running_mean(x, win_size):\n    return (np.cumsum(x)[win_size:] - np.cumsum(x[:-win_size])) / win_size\n\nfor (name, metrics), color in zip(result.items(),\n                                  matplotlib.rcParams['axes.prop_cycle'].by_key()['color']):\n    ax0.plot(\n        running_mean(metrics['train_loss'], 20),\n        color=color, label=name, alpha=0.8\n    )\n    ax0.plot(\n        np.linspace(0, len(metrics['train_loss']), len(metrics['test_loss']) + 1)[1:],\n        metrics['test_loss'], '--',\n        color=color, alpha=0.8\n    )\n    ax0.set_ylabel(\"Loss\")\n\n    ax1.plot(metrics['test_accuracy'], color=color, label=name)\n    ax1.set_ylabel(\"Test accuracy\")\n\nax1.legend();","pos":14,"state":"done","type":"cell"}
{"cell_type":"code","id":"68867e","input":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport numpy as np\n\n# selecting one of the 4 gpus randomly\ndevice = torch.device(f'cuda:{np.random.randint(4)}')\nprint(device)","pos":2,"state":"done","type":"cell"}
{"cell_type":"code","id":"6d4366","input":"# YOUR CODE HERE\nraise NotImplementedError()\n\n# E.g.:\n#\n#     model = ...\n#     result['MyModel'] = train_model(model, epochs=..., lr=...)\n#\n# (and then run the plotting code again)","metadata":{"nbgrader":{"cell_type":"code","checksum":"8e231cc1242e98cfc5a9857ebb35fb56","grade":false,"grade_id":"e12b7b","locked":false,"schema_version":3,"solution":true,"task":false}},"pos":16,"state":"done","type":"cell"}
{"cell_type":"code","id":"7f20d9","input":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\n# We are going to build a model from several convolutional blocks.\n# I.e. it's going to be:\n#\n#       [Conv2d -> Conv2d -> MaxPool2d] x 4\n#     \n# So why don't we define such a block as a separate Module?\nclass ConvBlock(nn.Module):\n    def __init__(self,\n                 in_channels,     # <== number of input channels to the 1st convolution\n                 interm_channels, # <== outputs of the 1st / inputs of the 2nd convolution\n                 out_channels,    # <== outputs of the 2nd convolution\n                 use_batchnorm,   # <== whether we'll use batchnorm\n                 initialization): # <== function that'll initialize the weights\n        # First we run the base class constructor\n        super(ConvBlock, self).__init__()\n\n        # And then define all the layers used within a block\n        self.conv1 = nn.Conv2d(in_channels=in_channels,\n                               out_channels=interm_channels,\n                               kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=interm_channels,\n                               out_channels=out_channels,\n                               kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n\n        self.use_batchnorm = use_batchnorm\n        if use_batchnorm:\n            self.bn1 = nn.BatchNorm2d(interm_channels)\n            self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # If initialization function provided, call it on the weights of the model\n        if initialization is not None:\n            initialization(self.conv1.weight)\n            initialization(self.conv2.weight)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        if self.use_batchnorm:\n            x = self.bn1(x)\n        x = F.relu(x)\n\n        x = self.conv2(x)\n        if self.use_batchnorm:\n            x = self.bn2(x)\n        x = F.relu(x)\n\n        x = self.pool(x)\n        return x\n\n# The model itself:\nclass Net(nn.Module):\n    def __init__(self, use_batchnorm, initialization):\n        super(Net, self).__init__()\n\n        # Convolutional layers:                                         # 3x32x32 (Channels x height x width)\n        self.conv1 = ConvBlock(3, 8, 16, use_batchnorm, initialization) # -> 8x32x32 -> 16x32x32 -> 16x16x16\n        self.conv2 = ConvBlock(16, 16, 32, use_batchnorm, initialization) # -> 16x16x16 -> 32x16x16 -> 32x8x8\n        self.conv3 = ConvBlock(32, 32, 64, use_batchnorm, initialization) # -> 32x8x8 -> 64x8x8 -> 64x4x4\n        self.conv4 = ConvBlock(64, 64, 128, use_batchnorm, initialization) # -> 64x4x4 -> 128x4x4 -> 128x2x2\n\n        # Fully connected layers:\n        self.fc1 = nn.Linear(128 * 2 * 2, 64)\n        self.fc2 = nn.Linear(64, 16)\n        self.fc3 = nn.Linear(16, 10)\n\n        # If initialization function provided, call it on the weights of the model\n        if initialization is not None:\n            initialization(self.fc1.weight)\n            initialization(self.fc2.weight)\n            initialization(self.fc3.weight)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n\n        x = x.view(x.shape[0], 128 * 2 * 2)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","pos":9,"state":"done","type":"cell"}
{"cell_type":"code","id":"8648bf","input":"model_device = next(model.parameters()).device\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for X, y in testloader:\n        X, y = X.to(model_device), y.to(model_device)\n\n        _, pred = torch.max(model(X).data, 1)\n\n        total += len(y)\n        correct += (pred == y).sum().item()\n\nprint('accuracy =', correct / total)\nassert correct / total >= 0.7","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"accuracy70","locked":true,"points":1,"schema_version":3,"solution":false,"task":false}},"pos":18,"state":"done","type":"cell"}
{"cell_type":"code","id":"9ecbc1","input":"model_device = next(model.parameters()).device\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for X, y in testloader:\n        X, y = X.to(model_device), y.to(model_device)\n\n        _, pred = torch.max(model(X).data, 1)\n\n        total += len(y)\n        correct += (pred == y).sum().item()\n\nprint('accuracy =', correct / total)\nassert correct / total >= 0.65","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"accuracy65","locked":true,"points":1,"schema_version":3,"solution":false,"task":false}},"pos":17,"state":"done","type":"cell"}
{"cell_type":"code","id":"c80f6c","input":"transform = transforms.ToTensor()\n\nBATCH_SIZE = 256\n\ntrainset = torchvision.datasets.CIFAR10(\n    root='../../../share/2.8_NetworkRegularization/data/',\n    train=True, download=True, transform=transform\n)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(\n    root='../../../share/2.8_NetworkRegularization/data/',\n    train=False, download=True, transform=transform\n)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=1024,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')","pos":5,"state":"done","type":"cell"}
{"cell_type":"code","id":"d97778","input":"import torch.optim as optim\nfrom tqdm import tqdm\n\nloss_fn = nn.CrossEntropyLoss() # softmax + neg. log likelihood\n\ndef train_model(model, epochs=3, lr=0.001):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    train_loss = []\n    test_loss = []\n    test_accuracy = []\n    for epoch in range(epochs):\n        model.train() # train mode (affects batchnorm layers:\n                      # in the subsequent forward passes they'll\n                      # exhibit 'train' behaviour, i.e. they'll\n                      # normalize activations over batches)\n        for i, (X, y) in enumerate(tqdm(trainloader)):\n            X, y = X.to(device), y.to(device)\n\n            pred = model(X)\n            loss = loss_fn(pred, y)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            train_loss.append(loss.item())\n\n        model.eval() # test mode (affects batchnorm layers:\n                     # in the subsequent forward passes they'll\n                     # exhibit 'test' behaviour, i.e. they'll\n                     # use the accumulated running statistics\n                     # to normalize activations)\n        epoch_losses = []\n        epoch_accuracies = []\n        with torch.no_grad(): # avoid calculating gradients during evaluation\n            for X, y in testloader:\n                X, y = X.to(device), y.to(device)\n\n                pred = model(X)\n\n                epoch_losses.append(loss_fn(pred, y).item())\n                _, pred = torch.max(pred.data, 1) # pred = index of maximal output along axis=1\n                epoch_accuracies.append(\n                    (pred == y).to(torch.float32).mean().item()\n                )\n        test_loss.append(np.mean(epoch_losses))\n        test_accuracy.append(np.mean(epoch_accuracies))\n\n    return dict(\n        train_loss=train_loss,\n        test_loss=test_loss,\n        test_accuracy=test_accuracy\n    )","pos":11,"state":"done","type":"cell"}
{"cell_type":"code","id":"e52c36","input":"model_device = next(model.parameters()).device\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for X, y in testloader:\n        X, y = X.to(model_device), y.to(model_device)\n\n        _, pred = torch.max(model(X).data, 1)\n\n        total += len(y)\n        correct += (pred == y).sum().item()\n\nprint('accuracy =', correct / total)\nassert correct / total >= 0.75","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"accuracy75","locked":true,"points":1,"schema_version":3,"solution":false,"task":false}},"pos":19,"state":"done","type":"cell"}
{"cell_type":"code","id":"f542f3","input":"configurations = dict(\n    fixed_normal_init=dict(\n        use_batchnorm=False,\n        initialization=(lambda w: w.data.normal_(std=0.001))\n    ),\n    he_normal_init=dict(\n        use_batchnorm=False,\n        initialization=(lambda w: torch.nn.init.kaiming_normal_(w, nonlinearity='relu'))\n    ),\n    he_normal_init_with_batchnorm=dict(\n        use_batchnorm=True,\n        initialization=(lambda w: torch.nn.init.kaiming_normal_(w, nonlinearity='relu'))\n    )\n)\n\n\n                                                 # the '**' notation transforms the dictionary\n                                                 # into keyword arguments, as if we called:\nresult = {                                       # Net(use_batchnorm=config['use_batchnorm'],\n    name : train_model(Net(**config).to(device)) #     initialization=config['initialization'])\n    for name, config in configurations.items()\n} # train the defined configurations, \n  # get the result as a dictionary","pos":13,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"034f92","input":"Note: if you get GPU out of memory errors later in the code and you believe your model isn't that heavy, check out the available GPU memory on other devices by invoking the command:\n\n```\n!nvidia-smi\n```\n\nThen, select the suitable GPU (0, 1, 2 or 3). E.g. if you want to select GPU 2, run this:\n\n```\ndevice = torch.device('cuda:2')\n```","pos":3,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"0cd811","input":"Some code borrowed from [this tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)","pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"0e775f","input":"## Getting the dataset (CIFAR-10)\n\nHere we'll download and open the data from the CIFAR-10 datset. (For more details see [this link](https://pytorch.org/docs/stable/torchvision/datasets.html).)","pos":4,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"17f713","input":"## Training","pos":10,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"409497","input":"# Regularization layers","pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4c3460","input":"## Experiments","pos":12,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"765336","input":"## Defining the model","pos":8,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"dc9af9","input":"## Your turn!\n\nTry improving the score. Since we don't have too much time to train the model thoroughly, see if you can change the model design and/or use regularization layers (batchnorm, dropout) to get better score quicker.","pos":15,"state":"done","type":"cell"}
{"id":0,"time":1595257996453,"type":"user"}
{"last_load":1595257995102,"type":"file"}